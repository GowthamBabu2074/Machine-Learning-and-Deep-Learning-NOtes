{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13]\n",
    "y = dataset.iloc[:, 13]\n",
    "#Create dummy variables\n",
    "geography=pd.get_dummies(X[\"Geography\"],drop_first=True)\n",
    "gender=pd.get_dummies(X['Gender'],drop_first=True)\n",
    "## Concatenate the Data Frames\n",
    "\n",
    "X=pd.concat([X,geography,gender],axis=1)\n",
    "\n",
    "## Drop Unnecessary columns\n",
    "X=X.drop(['Geography','Gender'],axis=1)\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 10)                120       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 241\n",
      "Trainable params: 241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Part 2 - Now let's make the ANN!\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "\n",
    "#glorot_normal, glorot_uniform, he_niform, he_normal, random_uniform, random_normal are different kernal_initializers\n",
    "classifier.add(Dense(units = 10, kernel_initializer='he_uniform', activation='relu', input_dim = 11))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform', activation='relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6536 - accuracy: 0.6477 - val_loss: 0.6196 - val_accuracy: 0.7247\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5850 - accuracy: 0.7595 - val_loss: 0.5694 - val_accuracy: 0.7747\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5410 - accuracy: 0.7886 - val_loss: 0.5330 - val_accuracy: 0.7948\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.5086 - accuracy: 0.8001 - val_loss: 0.5069 - val_accuracy: 0.8020\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.4843 - accuracy: 0.8031 - val_loss: 0.4879 - val_accuracy: 0.8046\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4664 - accuracy: 0.8065 - val_loss: 0.4741 - val_accuracy: 0.8061\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.8089 - val_loss: 0.4658 - val_accuracy: 0.8076\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.4456 - accuracy: 0.8108 - val_loss: 0.4602 - val_accuracy: 0.8095\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.8138 - val_loss: 0.4555 - val_accuracy: 0.8118\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4358 - accuracy: 0.8153 - val_loss: 0.4520 - val_accuracy: 0.8133\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4318 - accuracy: 0.8173 - val_loss: 0.4494 - val_accuracy: 0.8129\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4289 - accuracy: 0.8179 - val_loss: 0.4464 - val_accuracy: 0.8164\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.4265 - accuracy: 0.8199 - val_loss: 0.4439 - val_accuracy: 0.8186\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.4234 - accuracy: 0.8224 - val_loss: 0.4414 - val_accuracy: 0.8171\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.4211 - accuracy: 0.8240 - val_loss: 0.4391 - val_accuracy: 0.8179\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4182 - accuracy: 0.8246 - val_loss: 0.4367 - val_accuracy: 0.8194\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.4157 - accuracy: 0.8259 - val_loss: 0.4341 - val_accuracy: 0.8209\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.4128 - accuracy: 0.8272 - val_loss: 0.4312 - val_accuracy: 0.8194\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4100 - accuracy: 0.8300 - val_loss: 0.4286 - val_accuracy: 0.8209\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.4069 - accuracy: 0.8324 - val_loss: 0.4254 - val_accuracy: 0.8209\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.4033 - accuracy: 0.8345 - val_loss: 0.4223 - val_accuracy: 0.8209\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3995 - accuracy: 0.8367 - val_loss: 0.4183 - val_accuracy: 0.8224\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3955 - accuracy: 0.8380 - val_loss: 0.4141 - val_accuracy: 0.8243\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3908 - accuracy: 0.8405 - val_loss: 0.4098 - val_accuracy: 0.8262\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3860 - accuracy: 0.8412 - val_loss: 0.4052 - val_accuracy: 0.8289\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3812 - accuracy: 0.8434 - val_loss: 0.4005 - val_accuracy: 0.8300\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3769 - accuracy: 0.8449 - val_loss: 0.3967 - val_accuracy: 0.8353\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3729 - accuracy: 0.8455 - val_loss: 0.3926 - val_accuracy: 0.8379\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3691 - accuracy: 0.8475 - val_loss: 0.3892 - val_accuracy: 0.8387\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3653 - accuracy: 0.8489 - val_loss: 0.3853 - val_accuracy: 0.8421\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3614 - accuracy: 0.8500 - val_loss: 0.3817 - val_accuracy: 0.8432\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3574 - accuracy: 0.8528 - val_loss: 0.3784 - val_accuracy: 0.8440\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3539 - accuracy: 0.8537 - val_loss: 0.3751 - val_accuracy: 0.8440\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3506 - accuracy: 0.8541 - val_loss: 0.3726 - val_accuracy: 0.8463\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3482 - accuracy: 0.8574 - val_loss: 0.3706 - val_accuracy: 0.8455\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3459 - accuracy: 0.8576 - val_loss: 0.3687 - val_accuracy: 0.8470\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8593 - val_loss: 0.3673 - val_accuracy: 0.8470\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3418 - accuracy: 0.8599 - val_loss: 0.3658 - val_accuracy: 0.8470\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3399 - accuracy: 0.8595 - val_loss: 0.3650 - val_accuracy: 0.8489\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 921us/step - loss: 0.3389 - accuracy: 0.8606 - val_loss: 0.3639 - val_accuracy: 0.8512\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 973us/step - loss: 0.3370 - accuracy: 0.8608 - val_loss: 0.3627 - val_accuracy: 0.8538\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3358 - accuracy: 0.8604 - val_loss: 0.3622 - val_accuracy: 0.8516\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3349 - accuracy: 0.8610 - val_loss: 0.3615 - val_accuracy: 0.8538\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 964us/step - loss: 0.3335 - accuracy: 0.8614 - val_loss: 0.3606 - val_accuracy: 0.8554\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 943us/step - loss: 0.3327 - accuracy: 0.8602 - val_loss: 0.3601 - val_accuracy: 0.8561\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 985us/step - loss: 0.3321 - accuracy: 0.8617 - val_loss: 0.3598 - val_accuracy: 0.8550\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3316 - accuracy: 0.8628 - val_loss: 0.3592 - val_accuracy: 0.8554\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3308 - accuracy: 0.8623 - val_loss: 0.3596 - val_accuracy: 0.8580\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3295 - accuracy: 0.8621 - val_loss: 0.3591 - val_accuracy: 0.8576\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 984us/step - loss: 0.3293 - accuracy: 0.8630 - val_loss: 0.3587 - val_accuracy: 0.8576\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 967us/step - loss: 0.3286 - accuracy: 0.8615 - val_loss: 0.3588 - val_accuracy: 0.8565\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 986us/step - loss: 0.3283 - accuracy: 0.8632 - val_loss: 0.3587 - val_accuracy: 0.8599\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 987us/step - loss: 0.3274 - accuracy: 0.8645 - val_loss: 0.3583 - val_accuracy: 0.8599\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3274 - accuracy: 0.8651 - val_loss: 0.3581 - val_accuracy: 0.8588\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3269 - accuracy: 0.8642 - val_loss: 0.3589 - val_accuracy: 0.8595\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s 975us/step - loss: 0.3271 - accuracy: 0.8651 - val_loss: 0.3580 - val_accuracy: 0.8599\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s 984us/step - loss: 0.3267 - accuracy: 0.8643 - val_loss: 0.3579 - val_accuracy: 0.8607\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 987us/step - loss: 0.3259 - accuracy: 0.8647 - val_loss: 0.3578 - val_accuracy: 0.8607\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s 979us/step - loss: 0.3257 - accuracy: 0.8664 - val_loss: 0.3576 - val_accuracy: 0.8599\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3255 - accuracy: 0.8666 - val_loss: 0.3578 - val_accuracy: 0.8610\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3257 - accuracy: 0.8655 - val_loss: 0.3577 - val_accuracy: 0.8603\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3255 - accuracy: 0.8664 - val_loss: 0.3577 - val_accuracy: 0.8607\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3247 - accuracy: 0.8673 - val_loss: 0.3588 - val_accuracy: 0.8588\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 976us/step - loss: 0.3247 - accuracy: 0.8645 - val_loss: 0.3572 - val_accuracy: 0.8607\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s 969us/step - loss: 0.3242 - accuracy: 0.8673 - val_loss: 0.3573 - val_accuracy: 0.8599\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3243 - accuracy: 0.8658 - val_loss: 0.3574 - val_accuracy: 0.8599\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 954us/step - loss: 0.3243 - accuracy: 0.8670 - val_loss: 0.3583 - val_accuracy: 0.8603\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 969us/step - loss: 0.3241 - accuracy: 0.8668 - val_loss: 0.3575 - val_accuracy: 0.8603\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3238 - accuracy: 0.8666 - val_loss: 0.3572 - val_accuracy: 0.8603\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3235 - accuracy: 0.8664 - val_loss: 0.3570 - val_accuracy: 0.8618\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3234 - accuracy: 0.8673 - val_loss: 0.3569 - val_accuracy: 0.8595\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3234 - accuracy: 0.8655 - val_loss: 0.3569 - val_accuracy: 0.8607\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3230 - accuracy: 0.8675 - val_loss: 0.3570 - val_accuracy: 0.8591\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3235 - accuracy: 0.8656 - val_loss: 0.3570 - val_accuracy: 0.8610\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3228 - accuracy: 0.8671 - val_loss: 0.3568 - val_accuracy: 0.8603\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3226 - accuracy: 0.8658 - val_loss: 0.3568 - val_accuracy: 0.8610\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3228 - accuracy: 0.8664 - val_loss: 0.3572 - val_accuracy: 0.8603\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3228 - accuracy: 0.8662 - val_loss: 0.3566 - val_accuracy: 0.8610\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3228 - accuracy: 0.8681 - val_loss: 0.3569 - val_accuracy: 0.8599\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.8677 - val_loss: 0.3565 - val_accuracy: 0.8591\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8677 - val_loss: 0.3568 - val_accuracy: 0.8603\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8664 - val_loss: 0.3564 - val_accuracy: 0.8622\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3221 - accuracy: 0.8662 - val_loss: 0.3567 - val_accuracy: 0.8610\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8673 - val_loss: 0.3572 - val_accuracy: 0.8622\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.8666 - val_loss: 0.3562 - val_accuracy: 0.8607\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8675 - val_loss: 0.3563 - val_accuracy: 0.8626\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8692 - val_loss: 0.3566 - val_accuracy: 0.8618\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8688 - val_loss: 0.3569 - val_accuracy: 0.8622\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8684 - val_loss: 0.3561 - val_accuracy: 0.8614\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8688 - val_loss: 0.3568 - val_accuracy: 0.8610\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8681 - val_loss: 0.3564 - val_accuracy: 0.8603\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8679 - val_loss: 0.3558 - val_accuracy: 0.8626\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8684 - val_loss: 0.3563 - val_accuracy: 0.8607\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3210 - accuracy: 0.8684 - val_loss: 0.3569 - val_accuracy: 0.8610\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.8679 - val_loss: 0.3561 - val_accuracy: 0.8614\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8679 - val_loss: 0.3558 - val_accuracy: 0.8618\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3209 - accuracy: 0.8677 - val_loss: 0.3561 - val_accuracy: 0.8622\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3206 - accuracy: 0.8681 - val_loss: 0.3560 - val_accuracy: 0.8614\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8684 - val_loss: 0.3558 - val_accuracy: 0.8618\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.3209 - accuracy: 0.8686 - val_loss: 0.3561 - val_accuracy: 0.8614\n"
     ]
    }
   ],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "# Fitting the ANN to the Training set\n",
    "model_history = classifier.fit(X_train, y_train, validation_split=0.33, batch_size = 100, epochs = 100, \n",
    "                               callbacks = [stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "import matplotlib.pyplot as plt\n",
    "print(model_history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(model_history.history['accuracy'])\n",
    "plt.plot(model_history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 - Making the predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1505,   90],\n",
       "       [ 187,  218]], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8615"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred,y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras_Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13]\n",
    "y = dataset.iloc[:, 13]\n",
    "#Create dummy variables\n",
    "geography=pd.get_dummies(X[\"Geography\"],drop_first=True)\n",
    "gender=pd.get_dummies(X['Gender'],drop_first=True)\n",
    "## Concatenate the Data Frames\n",
    "\n",
    "X=pd.concat([X,geography,gender],axis=1)\n",
    "\n",
    "## Drop Unnecessary columns\n",
    "X=X.drop(['Geography','Gender'],axis=1)\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 11)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    for i in range(hp.Int('num_layers', 2, 20)):\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                            min_value=32,\n",
    "                                            max_value=512,\n",
    "                                            step=32),\n",
    "                               activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='project1',\n",
    "    project_name='Churn4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 48s]\n",
      "val_accuracy: 0.7975000143051147\n",
      "\n",
      "Best val_accuracy So Far: 0.7975000143051147\n",
      "Total elapsed time: 00h 03m 18s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs = 100,\n",
    "             validation_data=(X_test, y_test), callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kerastuner.engine.hyperparameters.HyperParameters at 0x27d0c35e940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "  1/200 [..............................] - ETA: 0s - loss: 3.5466 - accuracy: 0.9062WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0086s). Check your callbacks.\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6164 - accuracy: 0.7941 - val_loss: 0.5226 - val_accuracy: 0.7969\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5270 - accuracy: 0.7958 - val_loss: 0.5128 - val_accuracy: 0.7969\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5161 - accuracy: 0.7958 - val_loss: 0.5040 - val_accuracy: 0.7969\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7958 - val_loss: 0.5037 - val_accuracy: 0.7969\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5059 - accuracy: 0.7958 - val_loss: 0.5020 - val_accuracy: 0.7969\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5070 - accuracy: 0.7958 - val_loss: 0.5045 - val_accuracy: 0.7969\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5080 - accuracy: 0.7958 - val_loss: 0.5049 - val_accuracy: 0.7969\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5058 - accuracy: 0.7958 - val_loss: 0.5060 - val_accuracy: 0.7969\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5035 - accuracy: 0.7958 - val_loss: 0.4989 - val_accuracy: 0.7969\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7958 - val_loss: 0.4982 - val_accuracy: 0.7969\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5019 - accuracy: 0.7958 - val_loss: 0.4973 - val_accuracy: 0.7969\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5060 - accuracy: 0.7958 - val_loss: 0.4998 - val_accuracy: 0.7969\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.7958 - val_loss: 0.5003 - val_accuracy: 0.7969\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5036 - accuracy: 0.7958 - val_loss: 0.4999 - val_accuracy: 0.7969\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5037 - accuracy: 0.7958 - val_loss: 0.5024 - val_accuracy: 0.7969\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5056 - accuracy: 0.7958 - val_loss: 0.4992 - val_accuracy: 0.7969\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5008 - accuracy: 0.7958 - val_loss: 0.4992 - val_accuracy: 0.7969\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5036 - accuracy: 0.7958 - val_loss: 0.4962 - val_accuracy: 0.7969\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5003 - accuracy: 0.7958 - val_loss: 0.4957 - val_accuracy: 0.7969\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5010 - accuracy: 0.7958 - val_loss: 0.4958 - val_accuracy: 0.7969\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.7958 - val_loss: 0.4979 - val_accuracy: 0.7969\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5001 - accuracy: 0.7958 - val_loss: 0.4958 - val_accuracy: 0.7969\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4997 - accuracy: 0.7958 - val_loss: 0.4956 - val_accuracy: 0.7969\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4994 - accuracy: 0.7958 - val_loss: 0.4953 - val_accuracy: 0.7969\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4994 - accuracy: 0.7958 - val_loss: 0.5019 - val_accuracy: 0.7969\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4995 - accuracy: 0.7958 - val_loss: 0.4953 - val_accuracy: 0.7969\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4995 - accuracy: 0.7958 - val_loss: 0.4952 - val_accuracy: 0.7969\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.7958 - val_loss: 0.4949 - val_accuracy: 0.7969\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5067 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5049 - val_accuracy: 0.7969\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.7958 - val_loss: 0.5049 - val_accuracy: 0.7969\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.7958 - val_loss: 0.5049 - val_accuracy: 0.7969\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.7958 - val_loss: 0.5051 - val_accuracy: 0.7969\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5050 - val_accuracy: 0.7969\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7958 - val_loss: 0.5047 - val_accuracy: 0.7969\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7958 - val_loss: 0.5048 - val_accuracy: 0.7969\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5061 - accuracy: 0.7958 - val_loss: 0.5051 - val_accuracy: 0.7969\n",
      "Best epoch: 1\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6008 - accuracy: 0.7839 - val_loss: 0.5389 - val_accuracy: 0.7969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27d0d889cd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "hypermodel.fit(X_train, y_train, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5343 - accuracy: 0.7975\n",
      "[test loss, test accuracy]: [0.5343124270439148, 0.7975000143051147]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(X_test, y_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hypermodel.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1595,    0],\n",
       "       [ 405,    0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7975"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Real_Combine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,:-1] ## independent features\n",
    "y=df.iloc[:,-1] ## dependent features\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    for i in range(hp.Int('num_layers', 2, 20)):\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                            min_value=32,\n",
    "                                            max_value=512,\n",
    "                                            step=32),\n",
    "                               activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='mean_absolute_error',\n",
    "        metrics=['mean_absolute_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_mean_absolute_error',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='project',\n",
    "    project_name='Air Quality Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 14s]\n",
      "val_mean_absolute_error: 44.198317209879555\n",
      "\n",
      "Best val_mean_absolute_error So Far: 44.198317209879555\n",
      "Total elapsed time: 00h 01m 58s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs = 100,\n",
    "             validation_data=(X_test, y_test), callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kerastuner.engine.hyperparameters.HyperParameters at 0x27d159e7fa0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 74.1690 - mean_absolute_error: 74.1690 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 68.8351 - mean_absolute_error: 68.8351 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 68.8869 - mean_absolute_error: 68.8869 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 66.7118 - mean_absolute_error: 66.7118 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 66.7337 - mean_absolute_error: 66.7337 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 64.4252 - mean_absolute_error: 64.4252 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 65.0118 - mean_absolute_error: 65.0118 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 61.5496 - mean_absolute_error: 61.5496 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 59.7262 - mean_absolute_error: 59.7262 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 63.9055 - mean_absolute_error: 63.9055 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 50.9317 - mean_absolute_error: 50.9317 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 54.3359 - mean_absolute_error: 54.3359 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 54.8727 - mean_absolute_error: 54.8727 - val_loss: nan - val_mean_absolute_error: nan.529\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 52.8859 - mean_absolute_error: 52.8859 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 49.6336 - mean_absolute_error: 49.6336 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 45.8261 - mean_absolute_error: 45.8261 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 44.3321 - mean_absolute_error: 44.3321 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 46.6559 - mean_absolute_error: 46.6559 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 46.7488 - mean_absolute_error: 46.7488 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 43.5141 - mean_absolute_error: 43.5141 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 44.3658 - mean_absolute_error: 44.3658 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 43.8242 - mean_absolute_error: 43.8242 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 45.6420 - mean_absolute_error: 45.6420 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 43.3338 - mean_absolute_error: 43.3338 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 46.2451 - mean_absolute_error: 46.2451 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 45.5118 - mean_absolute_error: 45.5118 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 46.2989 - mean_absolute_error: 46.2989 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 42.6306 - mean_absolute_error: 42.6306 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 44.7759 - mean_absolute_error: 44.7759 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 47.0564 - mean_absolute_error: 47.0564 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 43.9532 - mean_absolute_error: 43.9532 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 42.2292 - mean_absolute_error: 42.2292 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 44.4284 - mean_absolute_error: 44.4284 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 42.2674 - mean_absolute_error: 42.2674 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 42.7745 - mean_absolute_error: 42.7745 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 46.9705 - mean_absolute_error: 46.9705 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 43.6095 - mean_absolute_error: 43.6095 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 41.8339 - mean_absolute_error: 41.8339 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 42.0522 - mean_absolute_error: 42.0522 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 43.1821 - mean_absolute_error: 43.1821 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 44.2719 - mean_absolute_error: 44.2719 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 45.8455 - mean_absolute_error: 45.8455 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 43.3520 - mean_absolute_error: 43.3520 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 42.8436 - mean_absolute_error: 42.8436 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 43.5585 - mean_absolute_error: 43.5585 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 41.7946 - mean_absolute_error: 41.7946 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 41.4234 - mean_absolute_error: 41.4234 - val_loss: nan - val_mean_absolute_error: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 41.5569 - mean_absolute_error: 41.5569 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 42.2107 - mean_absolute_error: 42.2107 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 42.7075 - mean_absolute_error: 42.7075 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 42.0856 - mean_absolute_error: 42.0856 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 44.2444 - mean_absolute_error: 44.2444 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 43.3191 - mean_absolute_error: 43.3191 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 41.3237 - mean_absolute_error: 41.3237 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 41.9512 - mean_absolute_error: 41.9512 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 41.5969 - mean_absolute_error: 41.5969 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 43.9839 - mean_absolute_error: 43.9839 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 42.9847 - mean_absolute_error: 42.9847 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 47.7982 - mean_absolute_error: 47.7982 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 41.0589 - mean_absolute_error: 41.0589 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 41.6687 - mean_absolute_error: 41.6687 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 42.2079 - mean_absolute_error: 42.2079 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 41.6112 - mean_absolute_error: 41.6112 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 42.5635 - mean_absolute_error: 42.5635 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 45.4065 - mean_absolute_error: 45.4065 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 41.2573 - mean_absolute_error: 41.2573 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 41.8550 - mean_absolute_error: 41.8550 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 43.5798 - mean_absolute_error: 43.5798 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 40.2013 - mean_absolute_error: 40.2013 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 40.5982 - mean_absolute_error: 40.5982 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 43.2107 - mean_absolute_error: 43.2107 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 42.5660 - mean_absolute_error: 42.5660 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 41.5904 - mean_absolute_error: 41.5904 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 41.8571 - mean_absolute_error: 41.8571 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 40.5406 - mean_absolute_error: 40.5406 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 41.1026 - mean_absolute_error: 41.1026 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 42.8127 - mean_absolute_error: 42.8127 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 42.3114 - mean_absolute_error: 42.3114 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 41.2179 - mean_absolute_error: 41.2179 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 40.3530 - mean_absolute_error: 40.3530 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 40.3008 - mean_absolute_error: 40.3008 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 39.9032 - mean_absolute_error: 39.9032 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 41.8792 - mean_absolute_error: 41.8792 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 40.4998 - mean_absolute_error: 40.4998 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 40.7943 - mean_absolute_error: 40.7943 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 41.6309 - mean_absolute_error: 41.6309 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 40.5324 - mean_absolute_error: 40.5324 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 40.5931 - mean_absolute_error: 40.5931 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 42.2522 - mean_absolute_error: 42.2522 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 41.3414 - mean_absolute_error: 41.3414 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 42.4415 - mean_absolute_error: 42.4415 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 42.4197 - mean_absolute_error: 42.4197 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 40.7145 - mean_absolute_error: 40.7145 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 41.0196 - mean_absolute_error: 41.0196 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 40.0609 - mean_absolute_error: 40.0609 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 39.9895 - mean_absolute_error: 39.9895 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 39.8443 - mean_absolute_error: 39.8443 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 40.9911 - mean_absolute_error: 40.9911 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 4ms/step - loss: 41.2311 - mean_absolute_error: 41.2311 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 40.0735 - mean_absolute_error: 40.0735 - val_loss: nan - val_mean_absolute_error: nan\n",
      "Best epoch: 1\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2)\n",
    "\n",
    "val_acc_per_epoch = history.history['mean_absolute_error']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 74.7733 - mean_absolute_error: 74.7733 - val_loss: nan - val_mean_absolute_error: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27d12c9c550>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "hypermodel.fit(X_train, y_train, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 1ms/step - loss: 65.0115 - mean_absolute_error: 65.0115\n",
      "[test loss, test accuracy]: [65.01148986816406, 65.01148986816406]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(X_test, y_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7230.21100954946"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = hypermodel.predict(X_test)\n",
    "final_mse = mean_squared_error(y_test, y_pred)\n",
    "final_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
